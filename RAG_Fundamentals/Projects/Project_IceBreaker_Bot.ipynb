{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce11a2a-bf77-4400-be62-9e9f46111f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gradio web interface for the Icebreaker Bot.\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import uuid\n",
    "import gradio as gr\n",
    "\n",
    "from modules.data_extraction import extract_linkedin_profile\n",
    "from modules.data_processing import split_profile_data, create_vector_database, verify_embeddings\n",
    "from modules.llm_interface import change_llm_model\n",
    "from modules.query_engine import generate_initial_facts, answer_user_query\n",
    "import config\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(stream=sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dictionary to store active conversations\n",
    "active_indices = {}\n",
    "\n",
    "def process_profile(linkedin_url, api_key, use_mock, selected_model):\n",
    "    \"\"\"Process a LinkedIn profile and generate initial facts.\n",
    "    \n",
    "    Args:\n",
    "        linkedin_url: LinkedIn profile URL to process.\n",
    "        api_key: ProxyCurl API key.\n",
    "        use_mock: Whether to use mock data.\n",
    "        selected_model: LLM model to use.\n",
    "        \n",
    "    Returns:\n",
    "        Initial facts about the profile and a session ID for this conversation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Change LLM model if needed\n",
    "        if selected_model != config.LLM_MODEL_ID:\n",
    "            change_llm_model(selected_model)\n",
    "            \n",
    "        # Use a default URL for mock data if none provided\n",
    "        if use_mock and not linkedin_url:\n",
    "            linkedin_url = \"https://www.linkedin.com/in/leonkatsnelson/\"\n",
    "            \n",
    "        # Extract profile data\n",
    "        profile_data = extract_linkedin_profile(\n",
    "            linkedin_url,\n",
    "            api_key if not use_mock else None,\n",
    "            mock=use_mock\n",
    "        )\n",
    "        \n",
    "        if not profile_data:\n",
    "            return \"Failed to retrieve profile data. Please check the URL or API key.\", None\n",
    "        \n",
    "        # Split data into nodes\n",
    "        nodes = split_profile_data(profile_data)\n",
    "        \n",
    "        if not nodes:\n",
    "            return \"Failed to process profile data into nodes.\", None\n",
    "        \n",
    "        # Create vector database\n",
    "        index = create_vector_database(nodes)\n",
    "        \n",
    "        if not index:\n",
    "            return \"Failed to create vector database.\", None\n",
    "        \n",
    "        # Verify embeddings\n",
    "        if not verify_embeddings(index):\n",
    "            logger.warning(\"Some embeddings may be missing or invalid\")\n",
    "        \n",
    "        # Generate initial facts\n",
    "        facts = generate_initial_facts(index)\n",
    "        \n",
    "        # Generate a unique session ID\n",
    "        session_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Store the index for this session\n",
    "        active_indices[session_id] = index\n",
    "        \n",
    "        # Return the facts and session ID\n",
    "        return f\"Profile processed successfully!\\n\\nHere are 3 interesting facts about this person:\\n\\n{facts}\", session_id\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_profile: {e}\")\n",
    "        return f\"Error: {str(e)}\", None\n",
    "\n",
    "def chat_with_profile(session_id, user_query, chat_history):\n",
    "    \"\"\"Chat with a processed LinkedIn profile.\n",
    "    \n",
    "    Args:\n",
    "        session_id: Session ID for this conversation.\n",
    "        user_query: User's question.\n",
    "        chat_history: Chat history.\n",
    "        \n",
    "    Returns:\n",
    "        Updated chat history.\n",
    "    \"\"\"\n",
    "    if not session_id:\n",
    "        return chat_history + [[user_query, \"No profile loaded. Please process a LinkedIn profile first.\"]]\n",
    "    \n",
    "    if session_id not in active_indices:\n",
    "        return chat_history + [[user_query, \"Session expired. Please process the LinkedIn profile again.\"]]\n",
    "    \n",
    "    if not user_query.strip():\n",
    "        return chat_history\n",
    "    \n",
    "    try:\n",
    "        # Get the index for this session\n",
    "        index = active_indices[session_id]\n",
    "        \n",
    "        # Answer the user's query\n",
    "        response = answer_user_query(index, user_query)\n",
    "        \n",
    "        # Update chat history\n",
    "        return chat_history + [[user_query, response.response]]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in chat_with_profile: {e}\")\n",
    "        return chat_history + [[user_query, f\"Error: {str(e)}\"]]\n",
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create the Gradio interface for the Icebreaker Bot.\"\"\"\n",
    "    # Define available LLM models\n",
    "    available_models = [\n",
    "        \"ibm/granite-3-2-8b-instruct\",\n",
    "        \"meta-llama/llama-3-3-70b-instruct\"\n",
    "    ]\n",
    "    \n",
    "    with gr.Blocks(title=\"LinkedIn Icebreaker Bot\") as demo:\n",
    "        gr.Markdown(\"# LinkedIn Icebreaker Bot\")\n",
    "        gr.Markdown(\"Generate personalized icebreakers and chat about LinkedIn profiles\")\n",
    "        \n",
    "        with gr.Tab(\"Process LinkedIn Profile\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    linkedin_url = gr.Textbox(\n",
    "                        label=\"LinkedIn Profile URL\",\n",
    "                        placeholder=\"https://www.linkedin.com/in/username/\"\n",
    "                    )\n",
    "                    api_key = gr.Textbox(\n",
    "                        label=\"ProxyCurl API Key (Leave empty to use mock data)\",\n",
    "                        placeholder=\"Your ProxyCurl API Key\",\n",
    "                        type=\"password\",\n",
    "                        value=config.PROXYCURL_API_KEY\n",
    "                    )\n",
    "                    use_mock = gr.Checkbox(label=\"Use Mock Data\", value=True)\n",
    "                    model_dropdown = gr.Dropdown(\n",
    "                        choices=available_models,\n",
    "                        label=\"Select LLM Model\",\n",
    "                        value=config.LLM_MODEL_ID\n",
    "                    )\n",
    "                    process_btn = gr.Button(\"Process Profile\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    result_text = gr.Textbox(label=\"Initial Facts\", lines=10)\n",
    "                    session_id = gr.Textbox(label=\"Session ID\", visible=False)\n",
    "            \n",
    "            process_btn.click(\n",
    "                fn=process_profile,\n",
    "                inputs=[linkedin_url, api_key, use_mock, model_dropdown],\n",
    "                outputs=[result_text, session_id]\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"Chat\"):\n",
    "            gr.Markdown(\"Chat with the processed LinkedIn profile\")\n",
    "            \n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            chat_input = gr.Textbox(\n",
    "                label=\"Ask a question about the profile\",\n",
    "                placeholder=\"What is this person's current job title?\"\n",
    "            )\n",
    "            \n",
    "            chat_btn = gr.Button(\"Send\")\n",
    "            \n",
    "            chat_btn.click(\n",
    "                fn=chat_with_profile,\n",
    "                inputs=[session_id, chat_input, chatbot],\n",
    "                outputs=[chatbot]\n",
    "            )\n",
    "            \n",
    "            chat_input.submit(\n",
    "                fn=chat_with_profile,\n",
    "                inputs=[session_id, chat_input, chatbot],\n",
    "                outputs=[chatbot]\n",
    "            )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    # Launch the Gradio interface\n",
    "    # You can customize these parameters:\n",
    "    # - share=True creates a public link you can share with others\n",
    "    # - server_name and server_port set where the app runs\n",
    "    demo.launch(\n",
    "        server_name=\"127.0.0.1\",  \n",
    "        server_port=5000,\n",
    "        share=True  # Set to False if you don't want to create a public link\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
