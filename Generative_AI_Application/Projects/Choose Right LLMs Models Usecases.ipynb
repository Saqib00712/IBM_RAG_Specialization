{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8824d-2732-40fd-9631-44ecc9976cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eed545-94de-4f2a-b4bf-2401717d4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "                   url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "                   # api_key = \"<YOUR_API_KEY>\" # Normally you'd put an API key here, but we've got you covered here\n",
    "                  )\n",
    "\n",
    "\n",
    "params = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"greedy\",\n",
    "    GenTextParamsMetaNames.MAX_NEW_TOKENS: 100\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a636add-dacf-4dd7-976d-42ae0629992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ModelInference(\n",
    "    model_id='ibm/granite-3-3-8b-instruct',\n",
    "    params=params,\n",
    "    credentials=credentials,\n",
    "    project_id=\"skills-network\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26e9af-8acc-42a7-b81a-484b442c7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Only reply with the answer. What is the capital of Canada?\n",
    "\"\"\"\n",
    "\n",
    "print(model.generate(text)['results'][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73beab1-07d1-43e7-b2f4-33616a760611",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelInference(\n",
    "    model_id='meta-llama/llama-4-maverick-17b-128e-instruct-fp8',\n",
    "    params=params,\n",
    "    credentials=credentials,\n",
    "    project_id=\"skills-network\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ba907-315f-478b-a65c-eb914bc4d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an expert assistant who provides concise and accurate answers.<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the capital of Canada?<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468c8fb-da09-4e43-8bfc-11fe958af147",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Flask langchain-ibm langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59276fe4-6ade-4aa8-94dd-1eff2ec27755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    # This is where we'll add our AI logic later\n",
    "    return jsonify({\"message\": \"AI response will be generated here\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699ca35-5ad4-449c-9702-d92a22ff1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# Model parameters\n",
    "PARAMETERS = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.MAX_NEW_TOKENS: 256,\n",
    "}\n",
    "\n",
    "# watsonx credentials\n",
    "# Note: Normally we'd need an API key, but in Skill's Network Cloud IDE will automatically handle that for you.\n",
    "CREDENTIALS = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"project_id\": \"skills-network\"\n",
    "}\n",
    "\n",
    "# Model IDs\n",
    "LLAMA_MODEL_ID = \"meta-llama/llama-3-2-11b-vision-instruct\"\n",
    "GRANITE_MODEL_ID = \"ibm/granite-3-3-8b-instruct\"\n",
    "MISTRAL_MODEL_ID = \"mistralai/mistral-small-3-1-24b-instruct-2503\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e7d78-1291-4691-a8b7-b1bdcc24f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import ChatWatsonx\n",
    "from langchain.prompts import PromptTemplate\n",
    "from config import PARAMETERS, LLAMA_MODEL_ID, GRANITE_MODEL_ID, MISTRAL_MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9358b05a-c586-4bf4-8e1c-8206875d7d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize a model\n",
    "def initialize_model(model_id):\n",
    "    return ChatWatsonx(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=PARAMETERS\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "llama_llm = initialize_model(LLAMA_MODEL_ID)\n",
    "granite_llm = initialize_model(GRANITE_MODEL_ID)\n",
    "mistral_llm = initialize_model(MISTRAL_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906774e-f1d9-4a05-9391-d2f9348f07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "llama_template = PromptTemplate(\n",
    "    template='''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "''',\n",
    "    input_variables=[\"system_prompt\", \"user_prompt\"]\n",
    ")\n",
    "\n",
    "granite_template = PromptTemplate(\n",
    "    template=\"<|system|>{system_prompt}\\n\\<|user|>{user_prompt}\\n<|assistant|>\",\n",
    "    input_variables=[\"system_prompt\", \"user_prompt\"]\n",
    ")\n",
    "\n",
    "mistral_template = PromptTemplate(\n",
    "    template=\"<s>[INST]{system_prompt}\\n{user_prompt}[/INST]\",\n",
    "    input_variables=[\"system_prompt\", \"user_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a5bba-df5f-4670-9290-f92cb617f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_response(model, template, system_prompt, user_prompt):\n",
    "    chain = template | model\n",
    "    return chain.invoke({'system_prompt':system_prompt, 'user_prompt':user_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6c991-2567-4ef8-9ba2-ec593ff24e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-specific response functions\n",
    "def llama_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)\n",
    "\n",
    "def granite_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(granite_llm, granite_template, system_prompt, user_prompt)\n",
    "\n",
    "def mistral_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(mistral_llm, mistral_template, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20743bf-308c-4e3f-9315-3a983ad00a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-specific response functions\n",
    "def llama_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)\n",
    "\n",
    "def granite_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(granite_llm, granite_template, system_prompt, user_prompt)\n",
    "\n",
    "def mistral_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(mistral_llm, mistral_template, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9159ac-700c-41eb-bf1d-fa43530a6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e17f89-0183-4a62-aecc-a528d9770fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define JSON output structure\n",
    "class AIResponse(BaseModel):\n",
    "    summary: str = Field(description=\"Summary of the user's message\")\n",
    "    sentiment: int = Field(description=\"Sentiment score from 0 (negative) to 100 (positive)\")\n",
    "    response: str = Field(description=\"Suggested response to the user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20af99-8015-4f4b-b82f-8c7a8e730627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON output parser\n",
    "json_parser = JsonOutputParser(pydantic_object=AIResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408423a3-7d73-47aa-938b-8dae3577c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_response(model, template, system_prompt, user_prompt):\n",
    "    chain = template | model | json_parser\n",
    "    return chain.invoke({'system_prompt':system_prompt, 'user_prompt':user_prompt, 'format_prompt':json_parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08210a6d-11a4-4637-929f-58c5fdb661b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_ibm import ChatWatsonx\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from config import PARAMETERS, CREDENTIALS, LLAMA_MODEL_ID, GRANITE_MODEL_ID, MISTRAL_MODEL_ID\n",
    "\n",
    "# Define JSON output structure\n",
    "class AIResponse(BaseModel):\n",
    "    summary: str = Field(description=\"Summary of the user's message\")\n",
    "    sentiment: int = Field(description=\"Sentiment score from 0 (negative) to 100 (positive)\")\n",
    "    response: str = Field(description=\"Suggested response to the user\")\n",
    "\n",
    "# JSON output parser\n",
    "json_parser = JsonOutputParser(pydantic_object=AIResponse)\n",
    "\n",
    "# Function to initialize a model\n",
    "def initialize_model(model_id):\n",
    "    return ChatWatsonx(\n",
    "        model_id=model_id,\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"skills-network\",\n",
    "        params=PARAMETERS\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "llama_llm = initialize_model(LLAMA_MODEL_ID)\n",
    "granite_llm = initialize_model(GRANITE_MODEL_ID)\n",
    "mistral_llm = initialize_model(MISTRAL_MODEL_ID)\n",
    "\n",
    "# Prompt templates\n",
    "llama_template = PromptTemplate(\n",
    "    template='''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_prompt}\\n{format_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "''',\n",
    "    input_variables=[\"system_prompt\", \"format_prompt\", \"user_prompt\"]\n",
    ")\n",
    "\n",
    "granite_template = PromptTemplate(\n",
    "    template=\"System: {system_prompt}\\n{format_prompt}\\nHuman: {user_prompt}\\nAI:\",\n",
    "    input_variables=[\"system_prompt\", \"format_prompt\", \"user_prompt\"]\n",
    ")\n",
    "\n",
    "mistral_template = PromptTemplate(\n",
    "        template=\"<s>[INST]{system_prompt}\\n{format_prompt}\\n{user_prompt}[/INST]\",\n",
    "    input_variables=[\"system_prompt\", \"format_prompt\", \"user_prompt\"]\n",
    ")\n",
    "\n",
    "def get_ai_response(model, template, system_prompt, user_prompt):\n",
    "    chain = template | model | json_parser\n",
    "    return chain.invoke({'system_prompt':system_prompt, 'user_prompt':user_prompt, 'format_prompt':json_parser.get_format_instructions()})\n",
    "\n",
    "# Model-specific response functions\n",
    "def llama_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)\n",
    "\n",
    "def granite_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(granite_llm, granite_template, system_prompt, user_prompt)\n",
    "\n",
    "def mistral_response(system_prompt, user_prompt):\n",
    "    return get_ai_response(mistral_llm, mistral_template, system_prompt, user_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
