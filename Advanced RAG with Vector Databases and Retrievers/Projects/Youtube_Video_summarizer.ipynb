{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f5edb-395b-4978-99f0-e87c29c94ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the YouTube bot\n",
    "import gradio as gr\n",
    "import re  #For extracting video id\n",
    "from youtube_transcript_api import YouTubeTranscriptApi  # For extracting transcripts from YouTube videos\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting text into manageable segments\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes  # For specifying model types\n",
    "from ibm_watsonx_ai import APIClient, Credentials  # For API client and credentials management\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams  # For managing model parameters\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods  # For defining decoding methods\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings  # For interacting with IBM's LLM and embeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils import get_embedding_model_specs  # For retrieving model specifications\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes  # For specifying types of embeddings\n",
    "from langchain_community.vectorstores import FAISS  # For efficient vector storage and similarity search\n",
    "from langchain.chains import LLMChain  # For creating chains of operations with LLMs\n",
    "from langchain.prompts import PromptTemplate  # For defining prompt templates\n",
    " \n",
    "def get_video_id(url):    \n",
    "    # Regex pattern to match YouTube video URLs\n",
    "    pattern = r'https:\\/\\/www\\.youtube\\.com\\/watch\\?v=([a-zA-Z0-9_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else None\n",
    " \n",
    "def get_transcript(url):\n",
    "    # Extracts the video ID from the URL\n",
    "    video_id = get_video_id(url)\n",
    " \n",
    "    # Create a YouTubeTranscriptApi() object\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "   \n",
    "    # Fetch the list of available transcripts for the given YouTube video\n",
    "    transcripts = ytt_api.list(video_id)\n",
    "   \n",
    "    transcript = \"\"\n",
    "    for t in transcripts:\n",
    "        # Check if the transcript's language is English\n",
    "        if t.language_code == 'en':\n",
    "            if t.is_generated:\n",
    "                # If no transcript has been set yet, use the auto-generated one\n",
    "                if len(transcript) == 0:\n",
    "                    transcript = t.fetch()\n",
    "            else:\n",
    "                # If a manually created transcript is found, use it (overrides auto-generated)\n",
    "                transcript = t.fetch()\n",
    "                break  # Prioritize the manually created transcript, exit the loop\n",
    "   \n",
    "    return transcript if transcript else None\n",
    " \n",
    " \n",
    "def process(transcript):\n",
    "    # Initialize an empty string to hold the formatted transcript\n",
    "    txt = \"\"\n",
    "   \n",
    "    # Loop through each entry in the transcript\n",
    "    for i in transcript:\n",
    "        try:\n",
    "            # Append the text and its start time to the output string\n",
    "            #txt += f\"Text: {i['text']} Start: {i['start']}\\n\"\n",
    "            txt += f\"Text: {i.text} Start: {i.start}\\n\"\n",
    "        except KeyError:\n",
    "            # If there is an issue accessing 'text' or 'start', skip this entry\n",
    "            pass\n",
    "           \n",
    "    # Return the processed transcript as a single string\n",
    "    return txt\n",
    " \n",
    "def chunk_transcript(processed_transcript, chunk_size=200, chunk_overlap=20):\n",
    "    # Initialize the RecursiveCharacterTextSplitter with specified chunk size and overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    " \n",
    "    # Split the transcript into chunks\n",
    "    chunks = text_splitter.split_text(processed_transcript)\n",
    "    return chunks\n",
    " \n",
    " \n",
    "def setup_credentials():\n",
    "    # Define the model ID for the WatsonX model being used\n",
    "    model_id = \"ibm/granite-3-2-8b-instruct\"\n",
    "   \n",
    "    # Set up the credentials by specifying the URL for IBM Watson services\n",
    "    credentials = Credentials(url=\"https://us-south.ml.cloud.ibm.com\")\n",
    "   \n",
    "    # Create an API client using the credentials\n",
    "    client = APIClient(credentials)\n",
    "   \n",
    "    # Define the project ID associated with the WatsonX platform\n",
    "    project_id = \"skills-network\"\n",
    "   \n",
    "    # Return the model ID, credentials, client, and project ID for later use\n",
    "    return model_id, credentials, client, project_id\n",
    " \n",
    "def define_parameters():\n",
    "    # Return a dictionary containing the parameters for the WatsonX model\n",
    "    return {\n",
    "        # Set the decoding method to GREEDY for generating text\n",
    "        GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "       \n",
    "        # Specify the maximum number of new tokens to generate\n",
    "        GenParams.MAX_NEW_TOKENS: 900,\n",
    "    }\n",
    " \n",
    " \n",
    "def initialize_watsonx_llm(model_id, credentials, project_id, parameters):\n",
    "    # Create and return an instance of the WatsonxLLM with the specified configuration\n",
    "    return WatsonxLLM(\n",
    "        model_id=model_id,          # Set the model ID for the LLM\n",
    "        url=credentials.get(\"url\"),      # Retrieve the service URL from credentials\n",
    "        project_id=project_id,            # Set the project ID for accessing resources\n",
    "        params=parameters                  # Pass the parameters for model behavior\n",
    "    )\n",
    " \n",
    " \n",
    " \n",
    "def setup_embedding_model(credentials, project_id):\n",
    "    # Create and return an instance of WatsonxEmbeddings with the specified configuration\n",
    "    return WatsonxEmbeddings(\n",
    "        model_id='ibm/slate-30m-english-rtrvr-v2',  # Set the model ID for the SLATE-30M embedding model\n",
    "        url=credentials[\"url\"],                            # Retrieve the service URL from the provided credentials\n",
    "        project_id=project_id                               # Set the project ID for accessing resources in the Watson environment\n",
    "    )\n",
    " \n",
    " \n",
    " \n",
    "def create_faiss_index(chunks, embedding_model):\n",
    "    \"\"\"\n",
    "    Create a FAISS index from text chunks using the specified embedding model.\n",
    "   \n",
    "    :param chunks: List of text chunks\n",
    "    :param embedding_model: The embedding model to use\n",
    "    :return: FAISS index\n",
    "    \"\"\"\n",
    "    # Use the FAISS library to create an index from the provided text chunks\n",
    "    return FAISS.from_texts(chunks, embedding_model)\n",
    " \n",
    " \n",
    " \n",
    "def perform_similarity_search(faiss_index, query, k=3):\n",
    "    \"\"\"\n",
    "    Search for specific queries within the embedded transcript using the FAISS index.\n",
    "   \n",
    "    :param faiss_index: The FAISS index containing embedded text chunks\n",
    "    :param query: The text input for the similarity search\n",
    "    :param k: The number of similar results to return (default is 3)\n",
    "    :return: List of similar results\n",
    "    \"\"\"\n",
    "    # Perform the similarity search using the FAISS index\n",
    "    results = faiss_index.similarity_search(query, k=k)\n",
    "    return results\n",
    " \n",
    " \n",
    "def create_summary_prompt():\n",
    "    \"\"\"\n",
    "    Create a PromptTemplate for summarizing a YouTube video transcript.\n",
    "   \n",
    "    :return: PromptTemplate object\n",
    "    \"\"\"\n",
    "    # Define the template for the summary prompt\n",
    "    template = \"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an AI assistant tasked with summarizing YouTube video transcripts. Provide concise, informative summaries that capture the main points of the video content.\n",
    " \n",
    "    Instructions:\n",
    "    1. Summarize the transcript in a single concise paragraph.\n",
    "    2. Ignore any timestamps in your summary.\n",
    "    3. Focus on the spoken content (Text) of the video.\n",
    " \n",
    "    Note: In the transcript, \"Text\" refers to the spoken words in the video, and \"start\" indicates the timestamp when that part begins in the video.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Please summarize the following YouTube video transcript:\n",
    " \n",
    "    {transcript}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "   \n",
    "    # Create the PromptTemplate object with the defined template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"transcript\"],\n",
    "        template=template\n",
    "    )\n",
    "   \n",
    "    return prompt\n",
    " \n",
    " \n",
    "def create_summary_chain(llm, prompt, verbose=True):\n",
    "    \"\"\"\n",
    "    Create an LLMChain for generating summaries.\n",
    "   \n",
    "    :param llm: Language model instance\n",
    "    :param prompt: PromptTemplate instance\n",
    "    :param verbose: Boolean to enable verbose output (default: True)\n",
    "    :return: LLMChain instance\n",
    "    \"\"\"\n",
    "    return LLMChain(llm=llm, prompt=prompt, verbose=verbose)\n",
    " \n",
    " \n",
    "def retrieve(query, faiss_index, k=7):\n",
    "    \"\"\"\n",
    "    Retrieve relevant context from the FAISS index based on the user's query.\n",
    " \n",
    "    Parameters:\n",
    "        query (str): The user's query string.\n",
    "        faiss_index (FAISS): The FAISS index containing the embedded documents.\n",
    "        k (int, optional): The number of most relevant documents to retrieve (default is 3).\n",
    " \n",
    "    Returns:\n",
    "        list: A list of the k most relevant documents (or document chunks).\n",
    "    \"\"\"\n",
    "    relevant_context = faiss_index.similarity_search(query, k=k)\n",
    "    return relevant_context\n",
    " \n",
    "def create_qa_prompt_template():\n",
    "    \"\"\"\n",
    "    Create a PromptTemplate for question answering based on video content.\n",
    "    Returns:\n",
    "        PromptTemplate: A PromptTemplate object configured for Q&A tasks.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Define the template string\n",
    "    qa_template = \"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert assistant providing detailed and accurate answers based on the following video content. Your responses should be:\n",
    "    1. Precise and free from repetition\n",
    "    2. Consistent with the information provided in the video\n",
    "    3. Well-organized and easy to understand\n",
    "    4. Focused on addressing the user's question directly\n",
    "    If you encounter conflicting information in the video content, use your best judgment to provide the most likely correct answer based on context.\n",
    "    Note: In the transcript, \"Text\" refers to the spoken words in the video, and \"start\" indicates the timestamp when that part begins in the video.<|eot_id|>\n",
    " \n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Relevant Video Context: {context}\n",
    "    Based on the above context, please answer the following question:\n",
    "    {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "    # Create the PromptTemplate object\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=qa_template\n",
    "    )\n",
    "    return prompt_template\n",
    " \n",
    " \n",
    "def create_qa_chain(llm, prompt_template, verbose=True):\n",
    "    \"\"\"\n",
    "    Create an LLMChain for question answering.\n",
    " \n",
    "    Args:\n",
    "        llm: Language model instance\n",
    "            The language model to use in the chain (e.g., WatsonxGranite).\n",
    "        prompt_template: PromptTemplate\n",
    "            The prompt template to use for structuring inputs to the language model.\n",
    "        verbose: bool, optional (default=True)\n",
    "            Whether to enable verbose output for the chain.\n",
    " \n",
    "    Returns:\n",
    "        LLMChain: An instantiated LLMChain ready for question answering.\n",
    "    \"\"\"\n",
    "   \n",
    "    return LLMChain(llm=llm, prompt=prompt_template, verbose=verbose)\n",
    " \n",
    " \n",
    "def generate_answer(question, faiss_index, qa_chain, k=7):\n",
    "    \"\"\"\n",
    "    Retrieve relevant context and generate an answer based on user input.\n",
    " \n",
    "    Args:\n",
    "        question: str\n",
    "            The user's question.\n",
    "        faiss_index: FAISS\n",
    "            The FAISS index containing the embedded documents.\n",
    "        qa_chain: LLMChain\n",
    "            The question-answering chain (LLMChain) to use for generating answers.\n",
    "        k: int, optional (default=3)\n",
    "            The number of relevant documents to retrieve.\n",
    " \n",
    "    Returns:\n",
    "        str: The generated answer to the user's question.\n",
    "    \"\"\"\n",
    " \n",
    "    # Retrieve relevant context\n",
    "    relevant_context = retrieve(question, faiss_index, k=k)\n",
    " \n",
    "    # Generate answer using the QA chain\n",
    "    answer = qa_chain.predict(context=relevant_context, question=question)\n",
    " \n",
    "    return answer\n",
    " \n",
    " \n",
    "# Initialize an empty string to store the processed transcript after fetching and preprocessing\n",
    "processed_transcript = \"\"\n",
    " \n",
    "def summarize_video(video_url):\n",
    "    \"\"\"\n",
    "    Title: Summarize Video\n",
    " \n",
    "    Description:\n",
    "    This function generates a summary of the video using the preprocessed transcript.\n",
    "    If the transcript hasn't been fetched yet, it fetches it first.\n",
    " \n",
    "    Args:\n",
    "        video_url (str): The URL of the YouTube video from which the transcript is to be fetched.\n",
    " \n",
    "    Returns:\n",
    "        str: The generated summary of the video or a message indicating that no transcript is available.\n",
    "    \"\"\"\n",
    "    global fetched_transcript, processed_transcript\n",
    "   \n",
    "   \n",
    "    if video_url:\n",
    "        # Fetch and preprocess transcript\n",
    "        fetched_transcript = get_transcript(video_url)\n",
    "        processed_transcript = process(fetched_transcript)\n",
    "    else:\n",
    "        return \"Please provide a valid YouTube URL.\"\n",
    " \n",
    "    if processed_transcript:\n",
    "        # Step 1: Set up IBM Watson credentials\n",
    "        model_id, credentials, client, project_id = setup_credentials()\n",
    " \n",
    "        # Step 2: Initialize WatsonX LLM for summarization\n",
    "        llm = initialize_watsonx_llm(model_id, credentials, project_id, define_parameters())\n",
    " \n",
    "        # Step 3: Create the summary prompt and chain\n",
    "        summary_prompt = create_summary_prompt()\n",
    "        summary_chain = create_summary_chain(llm, summary_prompt)\n",
    " \n",
    "        # Step 4: Generate the video summary\n",
    "        summary = summary_chain.run({\"transcript\": processed_transcript})\n",
    "        return summary\n",
    "    else:\n",
    "        return \"No transcript available. Please fetch the transcript first.\"\n",
    " \n",
    " \n",
    "def answer_question(video_url, user_question):\n",
    "    \"\"\"\n",
    "    Title: Answer User's Question\n",
    " \n",
    "    Description:\n",
    "    This function retrieves relevant context from the FAISS index based on the userâ€™s query\n",
    "    and generates an answer using the preprocessed transcript.\n",
    "    If the transcript hasn't been fetched yet, it fetches it first.\n",
    " \n",
    "    Args:\n",
    "        video_url (str): The URL of the YouTube video from which the transcript is to be fetched.\n",
    "        user_question (str): The question posed by the user regarding the video.\n",
    " \n",
    "    Returns:\n",
    "        str: The answer to the user's question or a message indicating that the transcript\n",
    "             has not been fetched.\n",
    "    \"\"\"\n",
    "    global fetched_transcript, processed_transcript\n",
    " \n",
    "    # Check if the transcript needs to be fetched\n",
    "    if not processed_transcript:\n",
    "        if video_url:\n",
    "            # Fetch and preprocess transcript\n",
    "            fetched_transcript = get_transcript(video_url)\n",
    "            processed_transcript = process(fetched_transcript)\n",
    "        else:\n",
    "            return \"Please provide a valid YouTube URL.\"\n",
    " \n",
    "    if processed_transcript and user_question:\n",
    "        # Step 1: Chunk the transcript (only for Q&A)\n",
    "        chunks = chunk_transcript(processed_transcript)\n",
    " \n",
    "        # Step 2: Set up IBM Watson credentials\n",
    "        model_id, credentials, client, project_id = setup_credentials()\n",
    " \n",
    "        # Step 3: Initialize WatsonX LLM for Q&A\n",
    "        llm = initialize_watsonx_llm(model_id, credentials, project_id, define_parameters())\n",
    " \n",
    "        # Step 4: Create FAISS index for transcript chunks (only needed for Q&A)\n",
    "        embedding_model = setup_embedding_model(credentials, project_id)\n",
    "        faiss_index = create_faiss_index(chunks, embedding_model)\n",
    " \n",
    "        # Step 5: Set up the Q&A prompt and chain\n",
    "        qa_prompt = create_qa_prompt_template()\n",
    "        qa_chain = create_qa_chain(llm, qa_prompt)\n",
    " \n",
    "        # Step 6: Generate the answer using FAISS index\n",
    "        answer = generate_answer(user_question, faiss_index, qa_chain)\n",
    "        return answer\n",
    "    else:\n",
    "        return \"Please provide a valid question and ensure the transcript has been fetched.\"\n",
    " \n",
    " \n",
    " \n",
    "with gr.Blocks() as interface:\n",
    " \n",
    "    gr.Markdown(\n",
    "        \"<h2 style='text-align: center;'>YouTube Video Summarizer and Q&A</h2>\"\n",
    "    )\n",
    " \n",
    "    # Input field for YouTube URL\n",
    "    video_url = gr.Textbox(label=\"YouTube Video URL\", placeholder=\"Enter the YouTube Video URL\")\n",
    "   \n",
    "    # Outputs for summary and answer\n",
    "    summary_output = gr.Textbox(label=\"Video Summary\", lines=5)\n",
    "    question_input = gr.Textbox(label=\"Ask a Question About the Video\", placeholder=\"Ask your question\")\n",
    "    answer_output = gr.Textbox(label=\"Answer to Your Question\", lines=5)\n",
    " \n",
    "    # Buttons for selecting functionalities after fetching transcript\n",
    "    summarize_btn = gr.Button(\"Summarize Video\")\n",
    "    question_btn = gr.Button(\"Ask a Question\")\n",
    " \n",
    "    # Display status message for transcript fetch\n",
    "    transcript_status = gr.Textbox(label=\"Transcript Status\", interactive=False)\n",
    " \n",
    "    # Set up button actions\n",
    "    summarize_btn.click(summarize_video, inputs=video_url, outputs=summary_output)\n",
    "    question_btn.click(answer_question, inputs=[video_url, question_input], outputs=answer_output)\n",
    " \n",
    "# Launch the app with specified server name and port\n",
    "interface.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
